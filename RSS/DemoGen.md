# DemoGen
## 摘要
DemoGen，一种低成本，完全合成的自动演示生成方法。DemoGen只使用一个人工收集的演示任务，通过将演示的动作轨迹调整到新的对象配置来生成空间增强的演示。利用3D点云作为模态，通过3D编辑重新安排场景中的主体，合成视觉观察结果。

在仅有 一个人工示范 的情况下，借助轨迹分段+任务与运动规划（TAMP）产生变换后的动作计划，并通过对 3D 点云进行空间编辑 来合成相应的视觉观测，从而低成本合成大量“空间增强”的示范，用以训练闭环的
视觉运动策略（visuomotor policy），显著提高策略的空间泛化能力。


## 介绍
为什么：
有限的空间泛化能力[41,43]。

减少冗余人力工作的一个潜在解决方案是用自动演示生成取代繁琐的重新定位和重新收集过程

    1. 最近的进展，如MimicGen[32]及其后续扩展[20,18,22]，已经提出通过基于物体相互作用分割演示轨迹来生成演示。
    2. 另一种选择是通过模拟到真实的传输进行部署[36,44,56]，尽管在机器人技术中弥合模拟到真实的差距仍然是一个重大挑战。

怎么解决：

DemoGen，这是一个数据生成系统，可以无缝地插入到模拟和物理世界的策略学习工作流程中。DemoGen认识到，在机器人上部署的高成本是实际部署的主要障碍，因此采用了一种完全合成的管道，有效地将生成的计划具体化为空间增强的演示，为训练政策做好准备。

对于动作生成，DemoGen通过结合任务和运动规划（Task and Motion Planning， TAMP）[10,5,31]中的技术来开发MimicGen策略，类似于最近发布的SkillMimicGen bb0中的实践。具体来说，我们将源轨迹分解为在自由空间中移动的运动段和通过接触进行非对象操作的技能段。在生成过程中，根据增强对象配置对技能段进行整体变换，变换后通过运动规划对运动段进行重新规划，连接相邻的技能段

DemoGen采用了一种更直接的策略：选择点云作为观测模式，通过3D编辑合成增强的视觉观测。

## 方法
DemoGen通过少量源演示生成空间增强的观察-动作对。针对动作部分，DemoGen将源轨迹解析为以物体为中心的运动和技能片段，并应用基于TAMP的适应机制。针对观察部分，DemoGen采用分
割与转换策略，高效合成机器人与物体的点云数据。

预处理：
采用单视角RGBD相机采集点云数据。原始点云观测数据首先经过预处理，剔除背景与桌面表面的冗余点。我们假设保留的点云数据均与被操作物体或机器人的末端执行器相关联。随后采用聚类操作[14]过滤真实世界观测中的异常点。接着通过最远点采样法将点云降采样至固定点数（如512或1024点），以促进策略学习[38]。 对于轨迹的首帧，我们采用Grounded SAM[40]从RGB图像中获取被操作物体的分割掩膜。这些掩膜随后被应用于像素对齐的深度图像，并投影至三维点云
解析源轨迹。遵循先前研究[32, 18]，我们假设执行轨迹可被解析为一系列以物体为中心的分段。考虑到机器人必须先在自由空间接近物体，再通过接触进行物体操作，每个以物体为中心的分段可进一步细分为两个阶段：运动阶段与技能阶段。例如图4所示任务中，轨迹被划分为四个阶段：1) 移动至花朵位置，2) 拾取花朵，3) 将花朵转移至花瓶，4) 将花朵插入花瓶。 通过检测物体点云几何中心与机器人末端执行器之间的距离是否在预设阈值内（如图4中球体所示），即可轻松识别与特定物体相关的技能阶段。两个技能阶段之间的中间轨迹则归类为运动阶段。

Tamp-based action gernaration:
假设我们有一条原始演示轨迹，比如“机器人拿起桌子中央的花瓶 → 放到右边”。如果现在桌子上
的花瓶初始位置变了（比如在左边），那原始动作轨迹就不能直接用了，需要重新变换/规划。
DemoGen 提出的处理方法是：

动作类型	处理方式
手部动作（比如抓紧/松开）	不变，因为抓紧就是抓紧，跟位置无关
机械臂动作（位姿轨迹）	必须按照物体的空间变换同步更新
更进一步，他们将原始轨迹分成两类：
轨迹段类型	含义	如何适配到新场景
Skill segments（技能	机器人与物体接触时的操作，比如	整体刚体变换，保持相对位置不变
段）	“抓住物体后移动”
Motion segments（运动段）	空中移动，与物体无接触	重新调用运动规划器（Motion Planner），规划新的路径

合成观测：
同步调整“机器人自身的感知状态（proprioception）”和“视觉点云”，使得整个轨迹在动力学和感
知上保持一致。
本体感知状态：手部不变 位姿改变
点云观测：
阶段名称	含义	如何处理其点云
To-do 阶段	物体还没被机器人碰	直接对点云做配置变换 (T0Ok)^(-1) * T0Ok′
Doing 阶段	物体正在被抓取/操作	与机器人末端执行器的点云合并为一体
Done 阶段	操作完成，物体静止	保持最终状态，不再变动

总结：
动作生成方法：基于 MimicGen 思想，将示范轨迹分解为“接触/技能段（skill segments）”与“空闲运动段（motion segments）”，将技能段直接按空间变换整体迁移，空闲段用运动规划（motion planning）重规划以连接技能段，融合了 TAMP 思路。

视觉合成策略：选择以 点云（3D point cloud） 为观察模态，通过识别物体/末端执行器对应的点云簇并对其施加与动作相同的空间变换来合成视觉观测，避免 2D 图像生成在透视/空间一致性上
的问题。

输入：单个真人示范轨迹（观测 + 动作） + 对象在工作台上目标空间的新的配置分布。
输出：若干条合成的（观测, 动作）示范轨迹用于策略训练。

流程：
• 将示范轨迹按接触点/交互段分段为 skill segments 与 free-space motion segments
• 为每个目标对象配置（即要生成的新物体位置），对 skill segments 应用相同的刚性位姿变换（保
• 留接触关系）。
• 对于 skill segments 之间的空闲段，调用运动规划器（motion planner）在变换后环境下重规划连接
• 路径（TAMP 风格），保证动作物理可行。
• 将动作计划映射到视觉模态：在原始点云中识别并分离出与物体及末端相关的点云簇，然后对这
• 些簇应用与动作相同的空间变换，合成新的点云观测。
• 输出（合成点云观测, 相应动作序列）对。使用这些合成示范训练闭环 visuomotor 策略（例如 3D 
• Diffusion Policy）并评估在整个工作区的泛化性能。

## 实验
仿真：使用DP3输入点云+本体状态 输出动作
手写策略脚本（Scripted）+ DemoGen
1 → 扩增为 100 或 200 条（取决于物体数）
测试结果大于10人工小于25人工
局限性：依赖分段点云 对不具有空间泛化的适应力差 视觉不匹配

未来研究：
1.可以探索如何减轻视觉不匹配的影响，例如利用对比学习或 3D 生成模型等技术。
2使用更多人工收集的演示作为源数据，旨在找到策略性能与数据收集总体成本之间的最佳平衡。
