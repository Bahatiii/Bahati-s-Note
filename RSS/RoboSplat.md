# RoboSplat
## 摘要
从遥操作演示中学习的视动策略面临诸多挑战，包括数据采集耗时长、成本高昂以及数据多样性不足等问题。现有方法通过增强RGB空间的图像观测或采用基于物理模拟器的实-模拟-实管道来解决这些问题。然而前者受限于二维数据增强，后者则因几何重建不精确导致物理模拟失真。
本文提出RoboSplat方法，通过直接操控三维高斯分布生成多样化且视觉逼真的演示场景。具体而言，我们通过三维高斯喷溅（3DGS）重建场景，直接编辑重建场景，并运用五种技术实现六类泛化数据增强：三维高斯替换技术处理不同物体类型、场景外观和机器人形态；等变变换技术处理不同物体姿态；视觉属性编辑技术处理多样光照条件；新型视角合成技术生成新摄像机视角；三维内容生成技术处理多样物体类型。

目标：从单次专家示范 + 多视角图像出发，利用 3D Gaussian Splatting（3DGS） 构建高保真场景表示，然后在 3D 上自动编辑以生成大量“真实感、空间一致”的示范数据，从而训练出在真实世界具有强鲁棒性的一次性模仿（one-shot）视觉-运动策略。

## 介绍
人工收集的演示数据非常耗时耗力，投入大量精力生成专家数据
2D图像增强容易但缺乏3D空间一致；Realtosimtoreal误差
3DGS 保留了现实世界的空间信息，并允许从多个视角进行一致的渲染，这使其成为模拟器图形引
擎在现实世界中的对应物，可用于生成新颖的演示。

RoboSplat：
一种使用高斯 Splatting 生成演示的新颖有效的方法。在 3DGS 的支持下，我们实现了操作场景的高保真重建。为了将重建的场景与现实世界的场景对齐，我们设计了一种利用高斯 Splatting 的可微分渲染的新型帧对齐管道。使用现成的分割模型和机器人联合机器人描述格式（URDF）对不同场景组件的 3D 高斯进行分割。单个收集的专家轨迹使我们能够在广泛的视觉领域生成新颖的演示。具体而言，RoboSplat 使用五种技术增强了六种泛化类型的数据：针对不同对象类型、场景外观和机器人实施例的 3D 高斯替换；不同物体姿势的等变变换；各种光照条件下的视觉属性编辑；新摄像机视角的新颖视图合成；以及针对不同对象类型的 3D 内容生成。


## 方法

输入：多视角 RGB 图像、一条专家演示轨迹（关键帧或连续末端位姿）、机器人 URDF。
输出：编辑好的场景、大量新生成的示范轨迹（对应的渲染图像序列 robot states 行为标签生成示范数据集）训练好的视觉-运动策略

流程：
相机位姿估计（COLMAP：输出相机参数、稀疏点云），得到每张图像的深度先验（估计出每个像素距离摄像机有多远），把图像 + 相机位姿 + 深度图输入 3D Gaussian Splatting（3DGS）→ 输出 3D 高斯表示场景
（此时机器人在3DGS自己的坐标系下还没对齐到真实URDF）

先ICP粗对齐，得到初始矩阵变换
再用可微分渲染精调把重建出的高斯模型精准对齐URDF，使后续对机器人关节、动作或场景物体的编辑变换能正确生效。

语义分割：
将整个场景的3D Gaussian集合拆分成每个对象或机器人的单元独立操作
在2D图像上用Grounded-SAM(图像分割模型 输出mask:二值图 1表示物体 0表示不属于物体) 将mask投影到3D高斯空间筛选出对应区域的Gaussian 
根据URDF，得出机器人link的点云 判断Gaussian的位置
最终拆成 机器人（连杆分开 可转） 物体 背景
最后利用分割后的高斯和变换能力生成新的示范：

局限：
无法处理可变形物体。此外，该流程缺乏物理约束，因此不适用于接触密集且动态的任务





## 相关工作：
Generalizable Policy in Robot Manipulation：可推广政策
Data Augmentation for Policy Learning：策略学习的数据增强 
Gaussian Splatting in Robotics：





