# Real2Render2Real
## 摘要
扩展机器人学习需要海量且多样化的数据集。然而，当前主流的数据采集模式——人类遥
操作——仍成本高昂，且受限于人工操作和实体机器人的物理访问。
我们提出Real2Render2Real（R2R2R）技术，这是一种无需依赖物体动力学模拟或机器人硬件遥操作即可生成训练数据的新方法。其输入仅需智能手机拍摄的一个或多个物体扫描数
据，以及一段人类操作演示视频（输入）。
R2R2R通过重建物体的精细三维几何结构与外观，并追踪六自由度运动轨迹，可渲染数千个高视觉保真度的机器人无关演示数据。该方法运用三维高斯溅射技术（3DGS）实现刚性与铰接物体的灵活资产生成及轨迹合成，并将这些表示形式转换为网格模型，确保与IsaacLab等可扩展渲染引擎兼容（碰撞建模功能关闭）。R2R2R生成的机器人演示数据可直接集成至基于机器人本体感知状态与图像观测的模型，如视觉-语言-动作模型（VLA）及模仿学习策略。物理实验表明，基于单次人类演示数据训练的模型，其性能可媲美基于150次人类遥操作演示训练的模型。


## 介紹：
我们推出了Real2Render2Real (R2R2R)，这是一种从智能手机物体扫描和人类演示视频生成大规模合成机器人训练数据的流程。R2R2R 在保持视觉精度的同时扩展了轨迹多样性：它使用物体姿势跟踪从视频中提取 6-DoF 物体部件轨迹，并在随机物体初始化下通过微分逆运动学生成相应的机器人执行。从多视图扫描开始，它重建 3D 物体几何和外观，通过部件级分解支持刚性和铰接物体，并使用 3D 高斯溅射来生成网格资产。生成的轨迹包括机器人本体感觉、末端执行器动作以及在不同光照、相机姿势和物体位置下渲染的成对 RGB 观测值——使它们与现代模仿学习策略（如视觉-语言-动作模型和扩散模型）直接兼容。通过消除对动力学模拟或机器人硬件的需求，R2R2R 实现了可访问且可扩展的机器人数据收集，任何人都可以通过使用智能手机捕捉日常物体交互来做出贡献。

本文有三点贡献。
首先，我们提出了Real2Render2Real (R2R2R)，这是一个新颖的框架，它仅使用智能手机拍摄的视频（多视角物体扫描和真人演示视频）即可合成多样化、基于物理基础的观察-动作对，而无需动力学模拟或机器人硬件。
其次，我们证明了这些数据与现代视觉-语言-动作 (VLA) 和模仿学习策略兼容，包括基于 Transformer 和基于扩散的架构，这些架构通过 RGB 和本体感受输入进行操作。
第三，我们表明，基于一次真人演示在 R2R2R 生成的数据上训练的策略，其性能可以媲美在 1,050 次物理机器人评估中基于 150 次真人遥控操作演示上训练的策略，同时所需的生成时间显著减少。
无需动力学仿真或机器人硬件、利用3D高斯点云重建技术生成训练数据、无需物体动力学仿真

## 方法
输入数据 3DGS并进行语义分割 对象重建
提取物体的六自由度轨迹，通过可微分渲染优化物体姿态使得渲染结果与真实视频匹配
物体部件嵌入预训练的DINO特征 使得姿态更加优化稳定
3DGS-to-mesh把物体从复杂背景中分离出 并进行部件分组
对物体轨迹进行插值生成多样化数据 
参考轨迹与轨迹重定向


## 相关工作：
Robot Data Collection
程序化数据生成
合成数据生成
Real2Sim2Real数据生成 使用 现实世界的观察结果来构建用于策略学习的模拟环境
