# DexFlyWheel
https://ar5iv.labs.arxiv.org/html/2509.23829

## 解决了什么问题：
Dexterous manipulation要求极高自由度和复杂的接触交互，需要多样化、大规模、高质量的数据集。而人工教学效率低，replay-based智能在原始演示的范围内进行空间变换，泛化差，纯RL训练困难，生成轨迹难以simtoreal。

## 论文提出：
一个可扩展的数据生成框架DexFlyWheel，能够高效从少量人工演示中生成大量多样化高质量的dexterous manipulation数据。

通过结合IL（从演示中学习类人行为）+residual RL残差强化（将先验知识适应新的场景学习生成类人且多样化的数据）+数据增强，构建一个自我改进的DexFlyWheel机制，这个闭环循环能够迭代地扩展数据多样性，最终训练出高泛化的策略。

## 如何解决：

![alt text](image-7.png)


框架概述：
 预热阶段 （左） 和自改进数据飞轮阶段 （右） 。在预热阶段，通过增强来自 VR 远程操作的种子演示数据，形成初始数据集 𝒟1 。数据飞轮阶段以闭环循环的方式运行，包含四个关键组件：（1）基础策略 πbase 训练，用于捕捉类人行为；（2）残差策略 πres 训练，用于增强泛化能力；（3）组合策略 πcombined 的部署，用于生成新的轨迹；（4）数据增强，用于进一步丰富数据集。随着飞轮的迭代，数据多样性和策略能力都会持续提升。

飞轮的迭代：
（1） 在数据集 𝒟i 上训练模仿学习策略 πbasei （当 i=1 时使用 𝒟1 ）。 （2） 为了提高对新对象的泛化能力，在冻结的 πbasei 的基础上训练残差强化学习策略 πresi ，得到组合策略 πcombinedi=πbasei+πresi 。 （3） 将组合策略部署到仿真环境中，生成各种对象配置下的演示数据，形成高质量的部署数据集 𝒟Oi 。 （4） 最后，通过环境和空间变化，利用 𝒜EP 进一步扩充 𝒟Oi ，生成用于下一次的数据集（数据增强与反馈）

## 独特性：
迭代的核心机制
策略结构：RL+IL




