# pointmappolicy
https://ar5iv.labs.arxiv.org/html/2510.20406

## 解决了什么问题：
现有操作策略单用RGB缺乏三维几何信息，单用点云难以利用图像领域成熟模型，因此难以在需要精细空间理解的长时序操控任务上既精确又高效的学习策略，因此，要设计一种既保持3D空间精确性又能用二维视觉方法处理数据的表示，从而提升多视角、多模态模仿学习在长horizon、精细操控任务上的性能与训练效率

## 论文提出：
把点云转成点云图“point map"（即与图像同尺寸、每像素带XYZ的2D网络），以便直接用成熟的二维视觉编码器与高效的序列模型来做多模态（RGB+3D）条件的扩散模仿策略

## 怎么解决：
1. point map :把每个摄像头的深度图用相机内参反投影为与RGB相同分辨率的三通道几何图（每像素为XYZ），再用外参把各视角映到同一世界参考系，得到规则化、无下采样且与像素对齐的3D表示;
反投影公式Unprojection、对太近太远活深度缺失的像素做mask、再用每个相机的外参把每个视角的M投到世界坐标系
2. 多模态tokenization：
用预训练CLIP文本编码器把指令编成向量token，对每视角图像用预训练的Film-resnet或其他预训练网络提取视觉embedding，再用单独的视觉编码器对point map做卷积得到per-view tokens，为每token加上可学习的位置embedding（视图索引、像素位置、时间步等）
3. 决策模型：EDM/score-based diffusion(决策方式) + x-LSTM backbone（backbone选择）
EDM：进行连续时间动作扩散生成动作训练神经网络再去噪

（这篇文章没咋看懂，回去补下深度学习）

## 独特性：
点云转为点云图

## 数据：
Benchmark：RoboCasa、CALVIN用于系统化评估多模态融合、泛化与长时序策略表现
