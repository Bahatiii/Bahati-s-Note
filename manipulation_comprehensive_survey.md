# Manpulationç»¼è¿°

https://arxiv.org/pdf/2510.10903

## Background
1. hardware

2. Non-learning: interpretability and safety in well-defined settings
   * Interpolation-based planning: e.g cubic splines perform well in lightweight and straightforward scenes, however limited adaptability to dynamic/uncertain environments
   * Sampling-based panning:
   * Optimization-based planning: offline/online 

   Learning-based: flexible generalization in complex or uncertain environments
   * MDP base
   * RL:
   * IL: BC: without an explicitly specified reward function , replaced by supervised learning ,mimick expert actions
   * IRL: inspect expert actions to learn the reward function ,not to mimick directly
   * GAILç”Ÿæˆå¼å¯¹æŠ—æ¨¡ä»¿å­¦ä¹ :

3. robotics models
   * VM: 
   * LM:
   * VLMs:
   * VAM VLAM:

4. evaluation:
   * Evaluation Metrics:success rate task completion time and action frequency return
   * Model selection: evaluate the model every ğ‘˜ epochs and select the checkpoint with the highest success rate.

## simulators benchmarks datasets
1. Grasping datasets:
   * rectangle-based :using a 5-dimensional grasp rectangle representation, defined by the center position (ğ‘¥, ğ‘¦), width and height, and the orientation angle relative to the horizontal axis.  
   * 6-DOF:(x,y,z)+(orientation)
2. Single-Embodiment Manipulation Simulators and Benchmarksï¼š
   * Basic Manipulation Benchmarks:basic:pick-and-place, sorting, pushing, inserting, opening, closing, and pouring. 
   * Further: multi-step operations, incorporating language prompts to guide trajectory generation,etc
   * Dexterous Manipulation benchmarks:
   * Deformable Object Manipulation Benchmarks:like cloth,rope,fluids

4. Trajectory Datasets: 
   * structured collections of time-ordered data that capture the sequential states, actions, and sensory observations of an agent interacting with an environment.include:robot joint states, end-effector poses, control inputs, and multimodal observations (e.g., RGB images, depth maps, force-torque readings) æ‹¥æœ‰ä»ä½ä¿çœŸåº¦çš„è¿œç¨‹æ“ä½œæ•°æ®åˆ°é«˜è´¨é‡çš„ä¸“å®¶æ¼”ç¤º,è®¸å¤šé«˜è´¨é‡çš„æ•°æ®é›†åŒ…æ‹¬è¯­ä¹‰æ ‡ç­¾ã€ä»»åŠ¡å®šä¹‰å’Œå¤šæ¨¡æ€è§‚
å¯Ÿï¼ˆtable4 æœ‰è®¸å¤šæ•°æ®é›†ï¼‰

5. Embodied QA and affourdance datasets
   * EQA:é¢å‘åŸºç¡€æ¨¡å‹çš„å…·èº«åŒ–å›ç­”ï¼Œè¦æ±‚ç†è§£ç¯å¢ƒä»¥å›ç­”å…³äºç¯å¢ƒçš„é—®é¢˜
   * Affordance Datasets:é’ˆå¯¹ä¸å¯¹è±¡çš„ä½çº§åŠŸèƒ½äº¤äº’ï¼Œå¦‚æŠ“å–æˆ–å·¥å…·ä½¿ç”¨
   They focun on:1.Visual Perception 2., Spatial Reasoningæœºå™¨äººä¸å…¶ç¯å¢ƒä¹‹é—´ç©ºé—´å…³ç³»çš„æ¨ç†ã€‚ä¾‹å¦‚ç¡®å®šæŠ“å–å™¨å’Œç›®æ ‡ç‰©ä½“ä¹‹é—´çš„ç›¸å¯¹æ–¹å‘ã€‚ 
   * Functional and Commonsense Reasoning:ç†è§£ç‰©ä½“çš„ç‰¹æ€§[159]å’ŒåŠŸèƒ½ç”¨é€”[153]ï¼Œä¾‹å¦‚ï¼Œè¯†åˆ«ç›˜å­çš„é­”æ–æ˜¯ç”¨æ¥æ¸…æ´é¤å…·çš„ï¼Œæˆ–è€…ä¸€æŠŠåˆ€åº”è¯¥ç”¨åˆ€æŸ„æŠ“ä½
## Manipulation tasks
1. grasping
   * =grasp detection and grasp generation,The primary focus is on predicting the position and orientation of the gripper 
   * These tasks involve identifying feasible grasp configurations from sensor inputs such as images or point clouds, allowing robotic end-effectors to securely pick up objects. 
   * Non-Learning-based Grasp:
   * Rectangle-based Grasp:
   * 6-DoF Grasp.:
   * Language-driven Grasp:1. multimodal feature fusion 2.leverages existing grasp models to generate large numbers of grasp candidates, followed by ranking or scoring with LLMs or VLMs to select the most confident grasps 3.directly fine-tunes MLLMs on grasp foundation models with large-scale grasp datasets
3. Dexterous Manipulation
   * =the capability of robotic systems equipped with multi-fingered or anthropomorphic hands to achieve precise and coordinated object control through complex contact interactions. 
   * Non-Learning-based Methods
   * Learning-based Methods.
4. Soft Robotic Manipulationï¼š
5. DOM:perceive and control non-rigid objects
6. Mobile Manipulationï¼š
7. Quadrupedal Manipulationï¼š
8. Humanoid Manipulation

## high-level planner
1. LLMs and multimodal LLMs (MLLMs) are increasingly used for task planning, code generation, and even motion planning
   * LLM-based Task Planning:
   * MLLM-based Task Planning:LLM only process textual information,while visual inputs are typically handlef by
3. Code generation:y introducing perception and control programming interfaces as prompts for an LLM, enabling the direct generation of executable code to govern robot behavior
5. Affordance as Planner:
   * Geometric Affordance.
   * Visual Affordance:directly learning interaction possibilities from 2D visual data, such as RGB or RGB-D images. Good:reducing the reliance on expert demonstrations. 
   * Semantic Affordance:how robots could acquire manipulation skills by associating semantic properties with observed human actions, like linking objects with trajectories
   * Multimodal Affordance:
6. 3D representation:
   * producing structured action proposals (e.g., 6-DoF grasps, relational rearrangements, or optimization costs)ç°åœ¨çš„å‘å±•è¶‹åŠ¿ï¼š1.editable,real-time Gaussian Splatting 2.implicit or descriptor fields that distill features from 2D foundation models into 3D for correspondence and language grounding.
   * Gaussian-splatting:
   * Physically Embodied GS couples 
   * particles (physics) to 3DGS (vision) for a real-time, visually-correctable world model

## low-level learning-based control
### learning strategy
Higher:While high-level planning determines what to do and in what order, such as task decomposition, skill sequencing, or goal reasoning, low-level control determines how to act by learning precise visuomotor mappings that enable robust manipulation in dynamic environments.
Divide to:1.input modeling(ç¡®å®šä½¿ç”¨ä»€ä¹ˆæ„ŸçŸ¥æ¨¡æ€å’Œç¼–ç æ–¹å¼) 2.latent learningï¼ˆå°†æ„ŸçŸ¥è½¬åŒ–ä¸ºå¯è¿ç§»çš„åµŒå…¥è¡¨ç¤ºï¼‰3.policy learning(å°†æ½œåœ¨è¡¨ç¤ºè§£ç ä¸ºå¯æ‰§è¡ŒåŠ¨ä½œ)
1. Learning strategy:RL:
   * RLï¼šBy leveraging high-dimensional perceptual inputs (e.g., vision or proprioception) and reward signals as feedback, RL enables agents to learn control policies through trial-and-error interaction with the environment.
   * model-free (ä¸ä¾èµ–ç¯å¢ƒåŠ¨åŠ›å­¦æ˜¾å¼æ¨¡å‹ï¼Œé€šè¿‡ä¸ç¯å¢ƒäº¤äº’ç›´æ¥å­¦ä¹ ):
   * RL in pre-training:QT-Optï¼šé€‚ç”¨äºè¿ç»­åŠ¨ä½œç©ºé—´ ä¸ºåŸºäºè§†è§‰çš„æœºå™¨äººæŠ“å–ä»»åŠ¡æä¾›å¯æ‰©å±•çš„è‡ªç›‘ç£å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå…ˆå­¦ä¹ ä¸€ä¸ªQå‡½æ•°ï¼Œå†ç”¨éšæœºä¼˜åŒ–ç®—æ³•æ›¿ä»£è´ªå©ªæœç´¢æ³•å¯»æ‰¾æœ€ä¼˜åŠ¨ä½œ
   * PTRï¼šç”¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ é¢„è®­ç»ƒï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿä»…å‡­å°‘é‡æ–°æ¼”ç¤ºå°±å¿«é€Ÿé€‚åº”æœªè§è¿‡çš„ç¯å¢ƒå’Œä»»åŠ¡
   * V-PTRï¼šåˆ©ç”¨äº’è”ç½‘ä¸Šæµ·é‡çš„äººç±»ç¤ºèŒƒæ•°æ®ã€‚é€šè¿‡è®­ç»ƒä»·å€¼å‡½æ•°æå–ç¨³å¥çš„ã€ä¸æ“ä½œç›¸å…³çš„è§†è§‰è¡¨å¾ï¼Œè¿›è€Œé’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚é™¤é¢„è®­ç»ƒç­–ç•¥ä¸ä»·å€¼å‡½æ•°å¤–ï¼Œé€šè¿‡é¢„è®­ç»ƒå¥–åŠ±å‡½æ•°äº¦å¯åŠ é€Ÿæ–°ä»»åŠ¡è®­ç»ƒã€‚
   * RL in fine-Tuning:
   * RL in VLA
   * Model-based Methods:
   * Imagination Trajectory Generation.
   * Planning.
   * Differentiable RL.
2. Learning strategy:IL:
  * ç›¸æ¯”è¾ƒRLé¿å…äº†æ˜‚è´µçš„å¥–åŠ±è®¾è®¡å’Œå¹¿æ³›çš„ç¯å¢ƒäº¤äº’ å¼•å…¥ResNet Transformerç­‰æ·±åº¦ç»“æ„ éšååŠ å…¥äº†å¤§æ¨¡å‹è§†è§‰ å¤šæ¨¡æ€é¢„è®­ç»ƒ LLM LVM
  * æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘åŒ…æ‹¬ï¼šå°†è§†è§‰å­¦ä¹ æ¶æ„ (VLA) æ¨¡å‹å’Œè¯­è¨€å­¦ä¹ æ¨¡å‹ (LLM) ä¸æœºå™¨äººæŠ€æœ¯ç›¸ç»“åˆçš„åŸºç¡€æ¨¡å‹é©±åŠ¨å‹æ™ºèƒ½å­¦ä¹  (IL)ï¼›åŸºäºè¡Œä¸ºæ¼”ç®—å’Œåäº‹å®æ¨ç†çš„å› æœæ™ºèƒ½å­¦ä¹ ï¼›è·¨ä»»åŠ¡å’Œè·¨å®ä¾‹çš„æ³›åŒ–å’Œè¿ç§»ï¼›é€šè¿‡è®¤è¯å’Œè¿è¡Œæ—¶ç›‘æ§å®ç°å®‰å…¨å¯é çš„éƒ¨ç½²ï¼›ä»¥åŠæ›´é«˜æ•ˆçš„äººæœºäº¤äº’ï¼Œä»¥å‡å°‘å­¦ä¹ è¿‡ç¨‹ä¸­å¯¹äººç±»åé¦ˆçš„ä¾èµ–
  * Imitation from action:
  * BC:æ ¹æ®ä¸“å®¶çŠ¶æ€-åŠ¨ä½œå¯¹ï¼Œå­¦ä¹ ä»çŠ¶æ€åˆ°åº•å±‚æ§åˆ¶æŒ‡ä»¤çš„ç›´æ¥æ˜ å°„
  * MP-based IL:
  * Search-based:
  * Optimization-based IL:
  * Reward-based IL.:recover an underlying reward or cost function that explains expert demonstrations
  * IRL(é€†å¼ºåŒ–å­¦ä¹ ):å¯»æ‰¾å¥–åŠ±æˆ–æˆæœ¬ä½¿ä¸“å®¶è½¨è¿¹æ¥è¿‘æœ€ä¼˜ï¼Œ
  * AIL:goalGAIL:è§£å†³ä¼ ç»Ÿç®—æ³•å¦‚BCåœ¨demonstrationsä¸­æ²¡æœ‰å‡ºç°çš„stateï¼Œagentçš„è¡¨ç°éå¸¸ä¸å¦‚äººæ„çš„é—®é¢˜
  * Latent Learning for ILï¼šæŠŠé«˜ç»´æ„ŸçŸ¥è¾“å…¥ï¼ˆå›¾åƒã€åŠ›/åŠ›çŸ©ã€å…³èŠ‚ä½å§¿ç­‰ï¼‰å‹ç¼©åˆ°ä½ç»´â€œæ½œåœ¨è¡¨ç¤ºï¼ˆlatentï¼‰â€ï¼Œå¹¶ç”¨è¿™äº›æ½œå˜é‡æ¥åšæ¨¡ä»¿å­¦ä¹ ï¼Œä»è€Œå‡å°‘å†—ä½™ã€å¼ºè°ƒä»»åŠ¡ç›¸å…³ä¿¡æ¯ã€æé«˜å¤šæ¨¡æ€å»ºæ¨¡ä¸å‡ ä½•ä¸€è‡´æ€§ï¼Œæœ€ç»ˆæ”¹å–„æ³›åŒ–å’Œæ•ˆç‡ã€‚
  * Imitation from observation:åªèƒ½è·å–ä¸“å®¶æ¼”ç¤ºçš„çŠ¶æ€æˆ–è§†è§‰è½¨è¿¹ï¼Œæ— æ³•è·å¾—ç›¸åº”çš„ä¸“å®¶åŠ¨ä½œæ ‡ç­¾ï¼Œå¿…é¡»ä»è§‚å¯Ÿåˆ°çš„çŠ¶æ€è½¬æ¢ï¼ˆä¾‹å¦‚è§†é¢‘ã€åŠ¨ä½œæ•æ‰ï¼‰ä¸­æ¨æ–­ç­–ç•¥åŸºäºè§‚å¯Ÿçš„å¥–åŠ±ï¼š
3. Learning with Auxiliary Tasksï¼š
  * World modelsï¼šæ ¸å¿ƒæ€æƒ³ï¼šæœºå™¨äººè‡ªå·±å­¦ä¼šâ€œé¢„æµ‹æœªæ¥ä¼šå‘ç”Ÿä»€ä¹ˆâ€é€šè¿‡è§‚å¯Ÿå’ŒåŠ¨ä½œå»é¢„æµ‹ä¸‹ä¸€æ­¥çŠ¶æ€ 
  * Generative Visual WMsï¼šå­¦ä¹ â€œè§†é¢‘æœªæ¥å¸§ç”Ÿæˆâ€
  * Image or Video Prediction
  * .Vision-Grounded Goal Extraction åŒ…æ‹¬Detection and Segmentation. 
  * Text-Grounded Goal Extraction
  * Contrastive Learning
  * Reconstruction:åŒ…æ‹¬ masked modeling+ generative reconstruction paradigmsæ©ç å»ºæ¨¡+ç”Ÿæˆå¼é‡å»º æ ¸å¿ƒæ€æƒ³ï¼šä»éƒ¨åˆ†è¾“å…¥ä¸­æ¢å¤æˆ–é¢„æµ‹ç¼ºå¤±æˆ–æ–°çš„è§‚æµ‹å€¼ï¼Œä¾‹å¦‚è¢«æ©ç çš„åƒç´ ã€æ ‡è®°æˆ–ä¸‰ç»´è¡¨å¾
  * 2D masked reconstruction:
  * 3D reconstruction:è¿˜ç”¨åˆ°äº†ç‚¹äº‘ã€å¤šè§†å›¾å›¾åƒç­‰

### Input Modeling
1. VAM:2D vision as input
2. VLAM:
  * 2D vision as input
  * Non-LLM-based VLA :ç›´æ¥æŠŠè§†è§‰è§‚å¯Ÿ + æ–‡æœ¬æŒ‡ä»¤ â†’ åºåˆ—åŒ–å»ºæ¨¡ â†’ ä½å±‚æ§åˆ¶åŠ¨ä½œä½†å¯¹æ•°æ®ä¾èµ–å¤§
  * LLM/VLM based VLA:èƒ½ç”¨è¾ƒå°‘çš„æ•°æ®å®ç°0-shot/less-shotçš„è¿ç§»   
3. Tactile-based Action Models
### latent learning
åˆ©ç”¨ç´§å‡‘ã€ç»“æ„åŒ–ä¸”å¯è¿ç§»çš„è¡¨å¾ï¼Œä»è€Œè¿æ¥æ„ŸçŸ¥å’Œæ§åˆ¶ã€‚å®ƒä¸“æ³¨äºå‘ç°èƒ½å¤Ÿæ•æ‰ä»»åŠ¡ç›¸
å…³è¯­ä¹‰ã€åŠ¨æ€å’Œå¯ä¾›æ€§ä¿¡æ¯çš„ä¸­é—´è¡¨å¾ï¼Œè¿›è€Œæå‡æ³›åŒ–èƒ½åŠ›å’Œæ ·æœ¬æ•ˆç‡
1. pretrained latent learning:é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒå­¦ä¹ é€šç”¨çš„è§†è§‰æˆ–å¤šæ¨¡æ€è¡¨å¾
æ ¹æ®è®­ç»ƒæ•°æ®çš„æ¥æºï¼Œæœºå™¨äººè¡¨å¾å¤§è‡´å¯ä»¥åˆ†ä¸ºä¸‰ç±»ï¼šåŸºäºé€šç”¨æ•°æ®é›†ï¼ˆä¾‹å¦‚ ImageNet[923]ï¼‰ã€ä»¥äººç±»ä¸ºä¸­å¿ƒçš„æ•°æ®é›†ï¼ˆä¾‹å¦‚ Ego4D[924]ï¼‰å’Œæœºå™¨äººä¸“ç”¨æ•°æ®é›†ï¼ˆä¾‹å¦‚ BridgeV2[145]ï¼‰è®­ç»ƒçš„è¡¨å¾ã€‚
2. latent action learning

### policy learning
1. æ—©æœŸï¼šMLP-based-policyåŸºäºå¤šå±‚æ„ŸçŸ¥å™¨
2. transformer-based policy  
3. DP:
  * å°†åŠ¨ä½œç”Ÿæˆé‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªå»å™ªè¿‡ç¨‹ï¼Œä»è€Œèƒ½å¤Ÿè¿›è¡Œå¤šæ¨¡æ€è½¨è¿¹é‡‡æ ·ï¼Œå¹¶åœ¨å„ç§æ“ä½œä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼ŒDP é¦–å…ˆå°†æœºå™¨äººè§†è§‰è¿åŠ¨æ§åˆ¶å»ºæ¨¡ä¸ºä¸€ä¸ªå»å™ªæ‰©æ•£è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡éšæœºæœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦å­¦ä¹ åŠ¨ä½œå¾—åˆ†æ¢¯åº¦ï¼Œä»è€Œå®ç°å¯¹å¤šæ¨¡æ€åŠ¨ä½œçš„é²æ£’å¤„ç†ï¼Œå¹¶å¼•å…¥æ»šåŠ¨æ—¶åŸŸæ§åˆ¶ã€è§†è§‰æ¡ä»¶åå°„å’Œæ—¶é—´åºåˆ—æ‰©æ•£å˜æ¢å™¨ï¼Œä»¥åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚
  * ä¸ºä»€ä¹ˆæœ‰ä¼˜åŠ¿ï¼šå¤šæ¨¡æ€åŠ¨ä½œï¼šæŠ“å–ã€æ’å…¥ç­‰ä»»åŠ¡ç»å¸¸æœ‰å¤šç§å¯è¡Œè½¨è¿¹ï¼Œæ‰©æ•£èƒ½é‡‡æ ·å¾—åˆ°å¤šç§å€™é€‰å¹¶é€‰æ‹©æœ€ä¼˜ï¼ˆæˆ–ç”¨èšåˆç­–ç•¥ï¼‰ã€‚é²æ£’æ€§ï¼šè¿­ä»£å»å™ªè¿‡ç¨‹å¯ä»¥æŠµæŠ—è¾“å…¥å™ªå£°ï¼Œå¹¶é€šè¿‡å¤šæ¬¡é‡‡æ ·æ‰¾åˆ°æ›´ç¨³å¥è§£ã€‚ä¸ planning çš„ç»“åˆè‡ªç„¶ï¼šå¯ä»¥åœ¨ latent/world model ä¸­åšå›æº¯é‡‡æ ·ä¸è¯„ä¼°ï¼Œé€‚åˆ receding-horizon / MPC å¼æ§åˆ¶ã€‚å¯ç»“åˆè§†è§‰ã€è§¦è§‰ã€3D ä¿¡æ¯ï¼šæ¡ä»¶åŒ–æœºåˆ¶æ˜“äºèåˆå¤šæ¨¡æ€è¾“å…¥ã€‚
  * 3DDPï¼šæŠŠåŠ¨ä½œã€è½¨è¿¹çš„æ¡ä»¶è½¬ä¸º3D token/ç‚¹äº‘ç‰¹å¾ï¼ˆDP3ç­‰ï¼‰
## key bottlenecks
1. data collection and utilization
  * human teleoperation and demonstration
  * human-in-the-loop enhancement
  * synthetic and automatic data:ä¸»è¦æ€è·¯å’Œæ–¹æ³•ï¼šåˆ©ç”¨LLM/VLMç”Ÿæˆä»»åŠ¡ã€æ¼”ç¤ºã€æ ‡æ³¨ï¼Œåˆ©ç”¨LLMæŠŠé«˜å±‚ä»»åŠ¡æ‹†æˆå­ä»»åŠ¡ï¼Œäº§ç”ŸæŒ‡ä»¤åºåˆ—æˆ–è€…ä¼ªä»£ç ï¼Œç”¨è§„åˆ’æœŸæˆ–RLæŠŠå­ä»»åŠ¡å˜æˆè½¨è¿¹æˆ–åŠ¨ä½œåºåˆ—ç”¨VLMå¯¹è§†é¢‘ã€æ¼”ç¤ºè‡ªåŠ¨ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°æˆ–åŠ¨ä½œæ ‡ç­¾ï¼ŒæŠŠç”Ÿæˆçš„è¯­è¨€ã€è„šæœ¬å†ç”¨ä½œæ¡ä»¶æ¥è®­ç»ƒ
  * Crowdsourcing-based Data Collection
  * data utilization data selection/retrieval/augmentation/expansion/reweighting

2. generalization
  * environment generalization:æ“æ§ç­–ç•¥çš„ç¯å¢ƒé€‚åº”åŠ› å…³é”®é—®é¢˜ï¼šsim-to-real-gapä¸è·¨åœºæ™¯ã€è®¾å¤‡æ³›åŒ–
  * Sim2real real2sim2real generalization:i.simultaion-only training åœ¨ä»¿çœŸé‡Œæ³¨å…¥å°½å¯èƒ½å¤šçš„å˜åŒ– æœŸæœ›è®­ç»ƒå‡ºçš„ç­–ç•¥èƒ½ç›´æ¥è¿ç§»åˆ°çœŸå®ä¸–ç•Œï¼Œä¸åšç°åœºå¾®è°ƒii. Real-world adaptation å…ˆåœ¨ä»¿çœŸä¸­è®­ç»ƒï¼Œå†ç”¨å°‘é‡çœŸå®æ•°æ®æ¨¡ä»¿è‡ªé€‚åº”
  * task generalization:i.for long-horizon tasks;Ii.compositional generalization;Iii.few-shot learning;v.meta learningå…ƒå­¦ä¹ 


